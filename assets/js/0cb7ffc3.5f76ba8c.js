"use strict";(self.webpackChunkopen_assistant=self.webpackChunkopen_assistant||[]).push([[5393],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>g});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),u=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},c=function(e){var t=u(e.components);return n.createElement(l.Provider,{value:t},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,c=o(e,["components","mdxType","originalType","parentName"]),p=u(a),m=r,g=p["".concat(l,".").concat(m)]||p[m]||d[m]||i;return a?n.createElement(g,s(s({ref:t},c),{},{components:a})):n.createElement(g,s({ref:t},c))}));function g(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,s=new Array(i);s[0]=m;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o[p]="string"==typeof e?e:r,s[1]=o;for(var u=2;u<i;u++)s[u]=a[u];return n.createElement.apply(null,s)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},93665:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>i,metadata:()=>o,toc:()=>u});var n=a(87462),r=(a(67294),a(3905));const i={},s="Supervised Datasets",o={unversionedId:"data/supervised-datasets",id:"data/supervised-datasets",title:"Supervised Datasets",description:"For discussion about usage of supervised data see issue",source:"@site/docs/data/supervised-datasets.md",sourceDirName:"data",slug:"/data/supervised-datasets",permalink:"/Open-Assistant/docs/data/supervised-datasets",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Data Augmentation",permalink:"/Open-Assistant/docs/data/augmentation"},next:{title:"Research",permalink:"/Open-Assistant/docs/research/"}},l={},u=[{value:"Motivation",id:"motivation",level:2},{value:"Promptsource",id:"promptsource",level:3},{value:"Natural instructions",id:"natural-instructions",level:3},{value:"Blended Skill Talk",id:"blended-skill-talk",level:3},{value:"SODA",id:"soda",level:3}],c={toc:u},p="wrapper";function d(e){let{components:t,...a}=e;return(0,r.kt)(p,(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"supervised-datasets"},"Supervised Datasets"),(0,r.kt)("p",null,"For discussion about usage of supervised data see issue\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/LAION-AI/Open-Assistant/issues/186"},"https://github.com/LAION-AI/Open-Assistant/issues/186"),"."),(0,r.kt)("h2",{id:"motivation"},"Motivation"),(0,r.kt)("p",null,"An important part of making the assistant useful is to teach it to understand\nand follow instructions, and to perform a large set of tasks well."),(0,r.kt)("p",null,"While RLHF seems like the main ingredient, using existing supervised data might\nhelp."),(0,r.kt)("p",null,"There are two large-scale projects in the area of instruction-following /\nmultitask learning: Promptsource and Natural Instructions - these projects\ncrowdsourced templates and turned existing NLP datasets into\ninstruction-following seq2seq form in natural language. They include both\nlong-output training examples like generating a sentence that is a likely\nconsequence of sentence in the prompt, and short-output, like rating prediction\nfrom review. (Pre-)training on such datasets should help model understand and\nfollow instructions and teach it many abilities necessary to perform a large set\nof tasks correctly. However, these data are not dialog-like - they do not look\nlike a normal conversation."),(0,r.kt)("p",null,'There are also supervised dialog datasets such as Blended Skill Talk or SODA. In\ncontrast to instruction-following datasets, dialog data is not as focused on\n"academic tasks" or correctness, but encourage the model to respond naturally\nlike a person would.'),(0,r.kt)("h3",{id:"promptsource"},"Promptsource"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"GitHub: ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/bigscience-workshop/promptsource"},"https://github.com/bigscience-workshop/promptsource")),(0,r.kt)("li",{parentName:"ul"},"paper:\n",(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2110.08207"},"Multitask Prompted Training Enables Zero-Shot Task Generalization")),(0,r.kt)("li",{parentName:"ul"},"project for preparing templates and working with them"),(0,r.kt)("li",{parentName:"ul"},"they generated a dataset using the templates:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/datasets/bigscience/P3"},"https://huggingface.co/datasets/bigscience/P3")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/datasets/bigscience/xP3"},"https://huggingface.co/datasets/bigscience/xP3")," (with multilingual data but\nEnglish prompt)"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/datasets/bigscience/xP3mt"},"https://huggingface.co/datasets/bigscience/xP3mt")," (with multilingual data\nand machine-translated prompt)"))),(0,r.kt)("li",{parentName:"ul"},"they trained zero-shot models (= models for following instructions in the\ninput)",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"based on T5 architecture (encoder-decoder) called T0 family (and MT0 for\nmultilingual)"),(0,r.kt)("li",{parentName:"ul"},"and based on GPT architecture (decoder-only) called BloomZ family"),(0,r.kt)("li",{parentName:"ul"},"Huggingface demo: ",(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/bigscience/T0pp"},"T0"),",\n",(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/bigscience/mt0-large"},"MT0"),",\n",(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/bigscience/bloomz"},"BloomZ"),","),(0,r.kt)("li",{parentName:"ul"},"GitHub repo for T0: ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/bigscience-workshop/t-zero"},"https://github.com/bigscience-workshop/t-zero")),(0,r.kt)("li",{parentName:"ul"},"GitHub repo for BloomZ and MT0:\n",(0,r.kt)("a",{parentName:"li",href:"https://github.com/bigscience-workshop/xmtf"},"https://github.com/bigscience-workshop/xmtf"))))),(0,r.kt)("h3",{id:"natural-instructions"},"Natural instructions"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"GitHub: ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/allenai/natural-instructions"},"https://github.com/allenai/natural-instructions")),(0,r.kt)("li",{parentName:"ul"},"paper:\n",(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2204.07705"},"Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks")),(0,r.kt)("li",{parentName:"ul"},"they crowdsource directly the data prepared for instruction following (and\nlearning from a few examples)"),(0,r.kt)("li",{parentName:"ul"},"the GitHub repo = the dataset. It contains jsons"),(0,r.kt)("li",{parentName:"ul"},"they trained zero-shot and in-context few-shot models (in multiple sizes):",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"mT5 architecture (encoder-decoder, multilingual pretraining)"),(0,r.kt)("li",{parentName:"ul"},"Huggingface demo few-shot:\n",(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/allenai/tk-instruct-3b-def-pos"},"https://huggingface.co/allenai/tk-instruct-3b-def-pos")),(0,r.kt)("li",{parentName:"ul"},"Huggingface demo zero-shot:\n",(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/allenai/tk-instruct-3b-def"},"https://huggingface.co/allenai/tk-instruct-3b-def"))))),(0,r.kt)("h3",{id:"blended-skill-talk"},"Blended Skill Talk"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"used by Facebook in Blenderbot project"),(0,r.kt)("li",{parentName:"ul"},"HuggingFace dataset: ",(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/datasets/blended_skill_talk"},"https://huggingface.co/datasets/blended_skill_talk")),(0,r.kt)("li",{parentName:"ul"},"example model trained on it:\n",(0,r.kt)("a",{parentName:"li",href:"https://huggingface.co/facebook/blenderbot_small-90M"},"https://huggingface.co/facebook/blenderbot_small-90M"))),(0,r.kt)("h3",{id:"soda"},"SODA"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"GitHub: ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/skywalker023/sodaverse"},"https://github.com/skywalker023/sodaverse")),(0,r.kt)("li",{parentName:"ul"},"paper: ",(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/2212.10465"},"https://arxiv.org/abs/2212.10465"))))}d.isMDXComponent=!0}}]);