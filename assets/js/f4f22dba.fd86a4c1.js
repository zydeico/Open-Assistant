"use strict";(self.webpackChunkopen_assistant=self.webpackChunkopen_assistant||[]).push([[2191],{3905:(e,n,a)=>{a.d(n,{Zo:()=>d,kt:()=>h});var t=a(67294);function i(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function r(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function o(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?r(Object(a),!0).forEach((function(n){i(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function s(e,n){if(null==e)return{};var a,t,i=function(e,n){if(null==e)return{};var a,t,i={},r=Object.keys(e);for(t=0;t<r.length;t++)a=r[t],n.indexOf(a)>=0||(i[a]=e[a]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)a=r[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=t.createContext({}),u=function(e){var n=t.useContext(l),a=n;return e&&(a="function"==typeof e?e(n):o(o({},n),e)),a},d=function(e){var n=u(e.components);return t.createElement(l.Provider,{value:n},e.children)},m="mdxType",g={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},c=t.forwardRef((function(e,n){var a=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),m=u(a),c=i,h=m["".concat(l,".").concat(c)]||m[c]||g[c]||r;return a?t.createElement(h,o(o({ref:n},d),{},{components:a})):t.createElement(h,o({ref:n},d))}));function h(e,n){var a=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=a.length,o=new Array(r);o[0]=c;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[m]="string"==typeof e?e:i,o[1]=s;for(var u=2;u<r;u++)o[u]=a[u];return t.createElement.apply(null,o)}return t.createElement.apply(null,a)}c.displayName="MDXCreateElement"},58604:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>g,frontMatter:()=>r,metadata:()=>s,toc:()=>u});var t=a(87462),i=(a(67294),a(3905));const r={},o="Research",s={unversionedId:"research/general",id:"research/general",title:"Research",description:"This page lists research papers that are relevant to the project.",source:"@site/docs/research/general.md",sourceDirName:"research",slug:"/research/general",permalink:"/Open-Assistant/docs/research/general",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Research",permalink:"/Open-Assistant/docs/research/"},next:{title:"Cohere Grounded QA",permalink:"/Open-Assistant/docs/research/search-based-qa"}},l={},u=[{value:"Table of Contents",id:"table-of-contents",level:2},{value:"Reinforcement Learning from Human Feedback",id:"reinforcement-learning-from-human-feedback",level:2},{value:"Fine-Tuning Language Models from Human Preferences [ArXiv], [GitHub]",id:"fine-tuning-language-models-from-human-preferences-arxiv-github",level:3},{value:"Learning to summarize from human feedback [ArXiv], [GitHub]",id:"learning-to-summarize-from-human-feedback-arxiv-github",level:3},{value:"Recursively Summarizing Books with Human Feedback [ArXiv]",id:"recursively-summarizing-books-with-human-feedback-arxiv",level:3},{value:"Training language models to follow instructions with human feedback [ArXiv]",id:"training-language-models-to-follow-instructions-with-human-feedback-arxiv",level:3},{value:"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback [ArXiv]",id:"training-a-helpful-and-harmless-assistant-with-reinforcement-learning-from-human-feedback-arxiv",level:3},{value:"Self-critiquing models for assisting human evaluators [ArXiv]",id:"self-critiquing-models-for-assisting-human-evaluators-arxiv",level:3},{value:"Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization [ArXiv]",id:"is-reinforcement-learning-not-for-natural-language-processing-benchmarks-baselines-and-building-blocks-for-natural-language-policy-optimization-arxiv",level:3},{value:"Generating Text From Language Models",id:"generating-text-from-language-models",level:2},{value:"RANKGEN: Improving Text Generation with Large Ranking Models [ArXiv], [GitHub]",id:"rankgen-improving-text-generation-with-large-ranking-models-arxiv-github",level:3},{value:"Automatically Generating Instruction Data for Training",id:"automatically-generating-instruction-data-for-training",level:2},{value:"SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions [ArXiv], [GitHub].",id:"self-instruct-aligning-language-model-with-self-generated-instructions-arxiv-github",level:3},{value:"Tuning Language Models with (Almost) No Human Labor. [ArXiv], [GitHub].",id:"tuning-language-models-with-almost-no-human-labor-arxiv-github",level:3},{value:"Uncertainty Estimation of Language Model Outputs",id:"uncertainty-estimation-of-language-model-outputs",level:2},{value:"Teaching models to express their uncertainty in words [ArXiv]",id:"teaching-models-to-express-their-uncertainty-in-words-arxiv",level:3},{value:"Evidence-Guided Text Generation",id:"evidence-guided-text-generation",level:2},{value:"WebGPT: Browser-assisted question-answering with human feedback [ArXiv]",id:"webgpt-browser-assisted-question-answering-with-human-feedback-arxiv",level:3},{value:"Teaching language models to support answers with verified quotes [ArXiv]",id:"teaching-language-models-to-support-answers-with-verified-quotes-arxiv",level:3},{value:"Reward Model Optimization",id:"reward-model-optimization",level:2},{value:"Scaling Laws for Reward Model Overoptimization [ArXiv], [Preceding Blogpost]",id:"scaling-laws-for-reward-model-overoptimization-arxiv-preceding-blogpost",level:3},{value:"Dialogue-Oriented RLHF",id:"dialogue-oriented-rlhf",level:2},{value:"Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning [ArXiv]",id:"dynamic-planning-in-open-ended-dialogue-using-reinforcement-learning-arxiv",level:3},{value:"Improving alignment of dialogue agents via targeted human judgements [ArXiv]",id:"improving-alignment-of-dialogue-agents-via-targeted-human-judgements-arxiv",level:3},{value:"Reduce Harms in Language Models",id:"reduce-harms-in-language-models",level:2},{value:"Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned [ArXiv]",id:"red-teaming-language-models-to-reduce-harms-methods-scaling-behaviors-and-lessons-learned-arxiv",level:3}],d={toc:u},m="wrapper";function g(e){let{components:n,...a}=e;return(0,i.kt)(m,(0,t.Z)({},d,a,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"research"},"Research"),(0,i.kt)("p",null,"This page lists research papers that are relevant to the project."),(0,i.kt)("h2",{id:"table-of-contents"},"Table of Contents"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Reinforcement Learning from Human Feedback"),(0,i.kt)("li",{parentName:"ul"},"Generating Text From Language Models"),(0,i.kt)("li",{parentName:"ul"},"Automatically Generating Instruction Data for Training"),(0,i.kt)("li",{parentName:"ul"},"Uncertainty Estimation of Language Model Outputs"),(0,i.kt)("li",{parentName:"ul"},"Evidence-Guided Text Generation"),(0,i.kt)("li",{parentName:"ul"},"Reward Model Optimization"),(0,i.kt)("li",{parentName:"ul"},"Dialogue-Oriented RLHF"),(0,i.kt)("li",{parentName:"ul"},"Reduce Harms in Language Models")),(0,i.kt)("h2",{id:"reinforcement-learning-from-human-feedback"},"Reinforcement Learning from Human Feedback"),(0,i.kt)("p",null,"Reinforcement Learning from Human Feedback (RLHF) is a method for fine-tuning a\ngenerative language models based on a reward model that is learned from human\npreference data. This method facilitates the learning of instruction-tuned\nmodels, among other things."),(0,i.kt)("h3",{id:"fine-tuning-language-models-from-human-preferences-arxiv-github"},"Fine-Tuning Language Models from Human Preferences [",(0,i.kt)("a",{parentName:"h3",href:"https://arxiv.org/abs/1909.08593"},"ArXiv"),"], [",(0,i.kt)("a",{parentName:"h3",href:"https://github.com/openai/lm-human-preferences"},"GitHub"),"]"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"In this paper, we build on advances in generative pretraining of language\nmodels to apply reward learning to four natural language tasks: continuing\ntext with positive sentiment or physically descriptive language, and\nsummarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic\ncontinuation we achieve good results with only 5,000 comparisons evaluated by\nhumans. For summarization, models trained with 60,000 comparisons copy whole\nsentences from the input but skip irrelevant preamble.")),(0,i.kt)("h3",{id:"learning-to-summarize-from-human-feedback-arxiv-github"},"Learning to summarize from human feedback [",(0,i.kt)("a",{parentName:"h3",href:"https://arxiv.org/abs/2009.01325"},"ArXiv"),"], [",(0,i.kt)("a",{parentName:"h3",href:"https://github.com/openai/summarize-from-feedback"},"GitHub"),"]"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"In this work, we show that it is possible to significantly improve summary\nquality by training a model to optimize for human preferences. We collect a\nlarge, high-quality dataset of human comparisons between summaries, train a\nmodel to predict the human-preferred summary, and use that model as a reward\nfunction to fine-tune a summarization policy using reinforcement learning.")),(0,i.kt)("h3",{id:"recursively-summarizing-books-with-human-feedback-arxiv"},"Recursively Summarizing Books with Human Feedback [",(0,i.kt)("a",{parentName:"h3",href:"https://arxiv.org/abs/2109.10862"},"ArXiv"),"]"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Our method combines learning from human feedback with recursive task\ndecomposition: we use models trained on smaller parts of the task to assist\nhumans in giving feedback on the broader task. We collect a large volume of\ndemonstrations and comparisons from human labelers. Our resulting model\ngenerates sensible summaries of entire books, even matching the quality of\nhuman-written summaries in a few cases (\u223c5% of books). We achieve\nstate-of-the-art results on the recent BookSum dataset for book-length\nsummarization. We release datasets of samples from our model.")),(0,i.kt)("h3",{id:"training-language-models-to-follow-instructions-with-human-feedback-arxiv"},"Training language models to follow instructions with human feedback [",(0,i.kt)("a",{parentName:"h3",href:"https://arxiv.org/abs/2203.02155"},"ArXiv"),"]"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Starting with a set of labeler-written prompts and prompts submitted through\nthe OpenAI API, we collect a dataset of labeler demonstrations of the desired\nmodel behavior, which we use to fine-tune GPT-3 using supervised learning. We\nthen collect a dataset of rankings of model outputs, which we use to further\nfine-tune this supervised model using reinforcement learning from human\nfeedback.")),(0,i.kt)("h3",{id:"training-a-helpful-and-harmless-assistant-with-reinforcement-learning-from-human-feedback-arxiv"},"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback [",(0,i.kt)("a",{parentName:"h3",href:"https://arxiv.org/abs/2204.05862"},"ArXiv"),"]"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"We apply preference modeling and reinforcement learning from human feedback\n(RLHF) to finetune language models to act as helpful and harmless assistants.\nWe find this alignment training improves performance on almost all NLP\nevaluations, and is fully compatible with training for specialized skills such\nas python coding and summarization.")),(0,i.kt)("h3",{id:"self-critiquing-models-for-assisting-human-evaluators-arxiv"},"Self-critiquing models for assisting human evaluators [",(0,i.kt)("a",{parentName:"h3",href:"https://arxiv.org/abs/2206.05802"},"ArXiv"),"]"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"We fine-tune large language models to write natural language critiques\n(natural language critical comments) using behavioral cloning. On a\ntopic-based summarization task, critiques written by our models help humans\nfind flaws in summaries that they would have otherwise missed. We study\nscaling properties of critiquing with both topic-based summarization and\nsynthetic tasks. Finally, we motivate and introduce a framework for comparing\ncritiquing ability to generation and discrimination ability. These results are\na proof of concept for using AI-assisted human feedback to scale the\nsupervision of machine learning systems to tasks that are difficult for humans\nto evaluate directly. We release our training datasets.")),(0,i.kt)("h3",{id:"is-reinforcement-learning-not-for-natural-language-processing-benchmarks-baselines-and-building-blocks-for-natural-language-policy-optimization-arxiv"},"Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization [",(0,i.kt)("a",{parentName:"h3",href:"https://arxiv.org/abs/2210.01241"},"ArXiv"),"]"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"We tackle the problem of aligning pre-trained large language models (LMs) with\nhuman preferences. We present the GRUE (General Reinforced-language\nUnderstanding Evaluation) benchmark, a set of 6 language generation tasks\nwhich are supervised by reward functions which capture automated measures of\nhuman preference. Finally, we introduce an easy-to-use, performant RL\nalgorithm, NLPO (Natural Language Policy Optimization) that learns to\neffectively reduce the combinatorial action space in language generation. We\nshow that RL techniques are generally better than supervised methods at\naligning LMs to human preferences.")),(0,i.kt)("h2",{id:"generating-text-from-language-models"},"Generating Text From Language Models"),(0,i.kt)("p",null,"A language model generates output text token by token, autoregressively. The\nlarge search space of this task requires some method of narrowing down the set\nof tokens to be considered in each step. This method, in turn, has a big impact\non the quality of the resulting text."),(0,i.kt)("h3",{id:"rankgen-improving-text-generation-with-large-ranking-models-arxiv-github"},"RANKGEN: Improving Text Generation with Large Ranking Models [",(0,i.kt)("a",{parentName:"h3",href:"https://arxiv.org/abs/2205.09726"},"ArXiv"),"], [",(0,i.kt)("a",{parentName:"h3",href:"https://github.com/martiansideofthemoon/rankgen"},"GitHub"),"]"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Given an input sequence (or prefix), modern language models often assign high\nprobabilities to output sequences that are repetitive, incoherent, or\nirrelevant to the prefix; as such, model-generated text also contains such\nartifacts. To address these issues we present RankGen, a 1.2B parameter\nencoder model for English that scores model generations given a prefix.\nRankGen can be flexibly incorporated as a scoring function in beam search and\nused to decode from any pretrained language model.")),(0,i.kt)("h2",{id:"automatically-generating-instruction-data-for-training"},"Automatically Generating Instruction Data for Training"),(0,i.kt)("p",null,"This line of work is about significantly reducing the need for manually\nannotated data for the purpose of training\n",(0,i.kt)("a",{parentName:"p",href:"https://openai.com/blog/instruction-following/"},"instruction-aligned")," language\nmodels."),(0,i.kt)("h3",{id:"self-instruct-aligning-language-model-with-self-generated-instructions-arxiv-github"},"SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions [",(0,i.kt)("a",{parentName:"h3",href:"https://arxiv.org/abs/2212.10560"},"ArXiv"),"], [",(0,i.kt)("a",{parentName:"h3",href:"https://github.com/yizhongw/self-instruct"},"GitHub"),"]."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"We introduce SELF-INSTRUCT, a framework for improving the\ninstruction-following capabilities of pretrained language models by\nbootstrapping off its own generations. Our pipeline generates instruction,\ninput, and output samples from a language model, then prunes them before using\nthem to finetune the original model. Applying our method to vanilla GPT3, we\ndemonstrate a 33% absolute improvement over the original model on\nSuperNaturalInstructions, on par with the performance of InstructGPT-0011,\nwhich is trained with private user data and human annotations.")),(0,i.kt)("h3",{id:"tuning-language-models-with-almost-no-human-labor-arxiv-github"},"Tuning Language Models with (Almost) No Human Labor. [",(0,i.kt)("a",{parentName:"h3",href:"https://arxiv.org/abs/2212.09689"},"ArXiv"),"], [",(0,i.kt)("a",{parentName:"h3",href:"https://github.com/orhonovich/unnatural-instructions"},"GitHub"),"]."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"In this work, we introduce Unnatural Instructions: a large dataset of creative\nand diverse instructions, collected with virtually no human labor. We collect\n64,000 examples by prompting a language model with three seed examples of\ninstructions and eliciting a fourth. This set is then expanded by prompting\nthe model to rephrase each instruction, creating a total of approximately\n240,000 examples of instructions, inputs, and outputs. Experiments show that\ndespite containing a fair amount of noise, training on Unnatural Instructions\nrivals the effectiveness of training on open-source manually-curated datasets,\nsurpassing the performance of models such as T0++ and Tk-Instruct across\nvarious benchmarks.")),(0,i.kt)("h2",{id:"uncertainty-estimation-of-language-model-outputs"},"Uncertainty Estimation of Language Model Outputs"),(0,i.kt)("h3",{id:"teaching-models-to-express-their-uncertainty-in-words-arxiv"},"Teaching models to express their uncertainty in words [",(0,i.kt)("a",{parentName:"h3",href:"https://arxiv.org/abs/2205.14334"},"ArXiv"),"]"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},'We show that a GPT-3 model can learn to express uncertainty about its own\nanswers in natural language -- without use of model logits. When given a\nquestion, the model generates both an answer and a level of confidence (e.g.\n"90% confidence" or "high confidence"). These levels map to probabilities that\nare well calibrated. The model also remains moderately calibrated under\ndistribution shift, and is sensitive to uncertainty in its own answers, rather\nthan imitating human examples.')),(0,i.kt)("h2",{id:"evidence-guided-text-generation"},"Evidence-Guided Text Generation"),(0,i.kt)("h3",{id:"webgpt-browser-assisted-question-answering-with-human-feedback-arxiv"},"WebGPT: Browser-assisted question-answering with human feedback [",(0,i.kt)("a",{parentName:"h3",href:"https://arxiv.org/abs/2112.09332"},"ArXiv"),"]"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"We fine-tune GPT-3 to answer long-form questions using a text-based\nweb-browsing environment, which allows the model to search and navigate the\nweb. We are able to train models on the task using imitation learning, and\nthen optimize answer quality with human feedback. Models must collect\nreferences while browsing in support of their answers. Our best model is\nobtained by fine-tuning GPT-3 using behavior cloning, and then performing\nrejection sampling against a reward model.")),(0,i.kt)("h3",{id:"teaching-language-models-to-support-answers-with-verified-quotes-arxiv"},"Teaching language models to support answers with verified quotes [",(0,i.kt)("a",{parentName:"h3",href:"https://arxiv.org/abs/2203.11147"},"ArXiv"),"]"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},'In this work we use RLHF to train "open-book" QA models that generate answers\nwhilst also citing specific evidence for their claims, which aids in the\nappraisal of correctness. Supporting evidence is drawn from multiple documents\nfound via a search engine, or from a single user-provided document. However,\nanalysis on the adversarial TruthfulQA dataset shows why citation is only one\npart of an overall strategy for safety and trustworthiness: not all claims\nsupported by evidence are true.')),(0,i.kt)("h2",{id:"reward-model-optimization"},"Reward Model Optimization"),(0,i.kt)("h3",{id:"scaling-laws-for-reward-model-overoptimization-arxiv-preceding-blogpost"},"Scaling Laws for Reward Model Overoptimization [",(0,i.kt)("a",{parentName:"h3",href:"https://arxiv.org/abs/2210.10760"},"ArXiv"),"], [",(0,i.kt)("a",{parentName:"h3",href:"https://openai.com/blog/measuring-goodharts-law/"},"Preceding Blogpost"),"]"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},'In this work, we use a synthetic setup in which a fixed "gold-standard" reward\nmodel plays the role of humans, providing labels used to train a proxy reward\nmodel. We study how the gold reward model score changes as we optimize against\nthe proxy reward model using either reinforcement learning or best-of-n\nsampling. We study the effect on this relationship of the size of the reward\nmodel dataset. We explore the implications of these empirical results for\ntheoretical considerations in AI alignment.')),(0,i.kt)("h2",{id:"dialogue-oriented-rlhf"},"Dialogue-Oriented RLHF"),(0,i.kt)("h3",{id:"dynamic-planning-in-open-ended-dialogue-using-reinforcement-learning-arxiv"},"Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning [",(0,i.kt)("a",{parentName:"h3",href:"https://arxiv.org/abs/2208.02294"},"ArXiv"),"]"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},'Building automated agents that can carry on rich open-ended conversations with\nhumans "in the wild" remains a formidable challenge. In this work we develop a\nreal-time, open-ended dialogue system that uses reinforcement learning (RL) to\npower a bot\'s conversational skill at scale. Trained using crowd-sourced data,\nour novel system is able to substantially exceeds several metrics of interest\nin a live experiment with real users of the Google Assistant.')),(0,i.kt)("h3",{id:"improving-alignment-of-dialogue-agents-via-targeted-human-judgements-arxiv"},"Improving alignment of dialogue agents via targeted human judgements [",(0,i.kt)("a",{parentName:"h3",href:"https://arxiv.org/abs/2209.14375"},"ArXiv"),"]"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"We present Sparrow, an information-seeking dialogue agent trained to be more\nhelpful, correct, and harmless compared to prompted language model baselines\nFirst, to make our agent more helpful and harmless, we break down the\nrequirements for good dialogue into natural language rules the agent should\nfollowy. Second, our agent provides evidence from sources supporting factual\nclaims when collecting preference judgements over model statements.Finally, we\nconduct extensive analyses showing that though our model learns to follow our\nrules it can exhibit distributional biases.")),(0,i.kt)("h2",{id:"reduce-harms-in-language-models"},"Reduce Harms in Language Models"),(0,i.kt)("h3",{id:"red-teaming-language-models-to-reduce-harms-methods-scaling-behaviors-and-lessons-learned-arxiv"},"Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned [",(0,i.kt)("a",{parentName:"h3",href:"https://arxiv.org/abs/2209.07858"},"ArXiv"),"]"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"We investigate scaling behaviors for red teaming. We find that the RLHF models\nare increasingly difficult to red team as they scale, and we find a flat trend\nwith scale for the other model types. We exhaustively describe our\ninstructions, processes, statistical methodologies, and uncertainty about red\nteaming.")))}g.isMDXComponent=!0}}]);