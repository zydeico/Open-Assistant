<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-research/general">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">Research | Open Assistant</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://LAION-AI.github.io/Open-Assistant/docs/research/general"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Research | Open Assistant"><meta data-rh="true" name="description" content="This page lists research papers that are relevant to the project."><meta data-rh="true" property="og:description" content="This page lists research papers that are relevant to the project."><link data-rh="true" rel="icon" href="/Open-Assistant/img/logo.svg"><link data-rh="true" rel="canonical" href="https://LAION-AI.github.io/Open-Assistant/docs/research/general"><link data-rh="true" rel="alternate" href="https://LAION-AI.github.io/Open-Assistant/docs/research/general" hreflang="en"><link data-rh="true" rel="alternate" href="https://LAION-AI.github.io/Open-Assistant/docs/research/general" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/Open-Assistant/blog/rss.xml" title="OpenAssistant Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/Open-Assistant/blog/atom.xml" title="OpenAssistant Blog Atom Feed">
<link rel="alternate" type="application/json" href="/Open-Assistant/blog/feed.json" title="OpenAssistant Blog JSON Feed"><link rel="stylesheet" href="/Open-Assistant/assets/css/styles.37a0cd52.css">
<link rel="preload" href="/Open-Assistant/assets/js/runtime~main.43d83866.js" as="script">
<link rel="preload" href="/Open-Assistant/assets/js/main.2f8790aa.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Open-Assistant/"><div class="navbar__logo"><img src="/Open-Assistant/img/logo.svg" alt="Open Assistant Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/Open-Assistant/img/logo.svg" alt="Open Assistant Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Open Assistant</b></a><a href="https://open-assistant.io/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">App<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/Open-Assistant/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/Open-Assistant/blog">Blog</a><a class="navbar__item navbar__link" href="/Open-Assistant/api">API</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/LAION-AI/Open-Assistant" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Open-Assistant/docs/intro">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/Open-Assistant/docs/guides">Guides</a><button aria-label="Toggle the collapsible sidebar category &#x27;Guides&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/Open-Assistant/docs/tasks">Tasks</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tasks&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/Open-Assistant/docs/data">Data</a><button aria-label="Toggle the collapsible sidebar category &#x27;Data&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/Open-Assistant/docs/research">Research</a><button aria-label="Toggle the collapsible sidebar category &#x27;Research&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/Open-Assistant/docs/research/general">Research</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Open-Assistant/docs/research/search-based-qa">Cohere Grounded QA</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Open-Assistant/docs/presentations">Presentations</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Open-Assistant/docs/faq">FAQ</a></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/Open-Assistant/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/Open-Assistant/docs/research"><span itemprop="name">Research</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Research</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Research</h1><p>This page lists research papers that are relevant to the project.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="table-of-contents">Table of Contents<a href="#table-of-contents" class="hash-link" aria-label="Direct link to Table of Contents" title="Direct link to Table of Contents">​</a></h2><ul><li>Reinforcement Learning from Human Feedback</li><li>Generating Text From Language Models</li><li>Automatically Generating Instruction Data for Training</li><li>Uncertainty Estimation of Language Model Outputs</li><li>Evidence-Guided Text Generation</li><li>Reward Model Optimization</li><li>Dialogue-Oriented RLHF</li><li>Reduce Harms in Language Models</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="reinforcement-learning-from-human-feedback">Reinforcement Learning from Human Feedback<a href="#reinforcement-learning-from-human-feedback" class="hash-link" aria-label="Direct link to Reinforcement Learning from Human Feedback" title="Direct link to Reinforcement Learning from Human Feedback">​</a></h2><p>Reinforcement Learning from Human Feedback (RLHF) is a method for fine-tuning a
generative language models based on a reward model that is learned from human
preference data. This method facilitates the learning of instruction-tuned
models, among other things.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="fine-tuning-language-models-from-human-preferences-arxiv-github">Fine-Tuning Language Models from Human Preferences [<a href="https://arxiv.org/abs/1909.08593" target="_blank" rel="noopener noreferrer">ArXiv</a>], [<a href="https://github.com/openai/lm-human-preferences" target="_blank" rel="noopener noreferrer">GitHub</a>]<a href="#fine-tuning-language-models-from-human-preferences-arxiv-github" class="hash-link" aria-label="Direct link to fine-tuning-language-models-from-human-preferences-arxiv-github" title="Direct link to fine-tuning-language-models-from-human-preferences-arxiv-github">​</a></h3><blockquote><p>In this paper, we build on advances in generative pretraining of language
models to apply reward learning to four natural language tasks: continuing
text with positive sentiment or physically descriptive language, and
summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic
continuation we achieve good results with only 5,000 comparisons evaluated by
humans. For summarization, models trained with 60,000 comparisons copy whole
sentences from the input but skip irrelevant preamble.</p></blockquote><h3 class="anchor anchorWithStickyNavbar_LWe7" id="learning-to-summarize-from-human-feedback-arxiv-github">Learning to summarize from human feedback [<a href="https://arxiv.org/abs/2009.01325" target="_blank" rel="noopener noreferrer">ArXiv</a>], [<a href="https://github.com/openai/summarize-from-feedback" target="_blank" rel="noopener noreferrer">GitHub</a>]<a href="#learning-to-summarize-from-human-feedback-arxiv-github" class="hash-link" aria-label="Direct link to learning-to-summarize-from-human-feedback-arxiv-github" title="Direct link to learning-to-summarize-from-human-feedback-arxiv-github">​</a></h3><blockquote><p>In this work, we show that it is possible to significantly improve summary
quality by training a model to optimize for human preferences. We collect a
large, high-quality dataset of human comparisons between summaries, train a
model to predict the human-preferred summary, and use that model as a reward
function to fine-tune a summarization policy using reinforcement learning.</p></blockquote><h3 class="anchor anchorWithStickyNavbar_LWe7" id="recursively-summarizing-books-with-human-feedback-arxiv">Recursively Summarizing Books with Human Feedback [<a href="https://arxiv.org/abs/2109.10862" target="_blank" rel="noopener noreferrer">ArXiv</a>]<a href="#recursively-summarizing-books-with-human-feedback-arxiv" class="hash-link" aria-label="Direct link to recursively-summarizing-books-with-human-feedback-arxiv" title="Direct link to recursively-summarizing-books-with-human-feedback-arxiv">​</a></h3><blockquote><p>Our method combines learning from human feedback with recursive task
decomposition: we use models trained on smaller parts of the task to assist
humans in giving feedback on the broader task. We collect a large volume of
demonstrations and comparisons from human labelers. Our resulting model
generates sensible summaries of entire books, even matching the quality of
human-written summaries in a few cases (∼5% of books). We achieve
state-of-the-art results on the recent BookSum dataset for book-length
summarization. We release datasets of samples from our model.</p></blockquote><h3 class="anchor anchorWithStickyNavbar_LWe7" id="training-language-models-to-follow-instructions-with-human-feedback-arxiv">Training language models to follow instructions with human feedback [<a href="https://arxiv.org/abs/2203.02155" target="_blank" rel="noopener noreferrer">ArXiv</a>]<a href="#training-language-models-to-follow-instructions-with-human-feedback-arxiv" class="hash-link" aria-label="Direct link to training-language-models-to-follow-instructions-with-human-feedback-arxiv" title="Direct link to training-language-models-to-follow-instructions-with-human-feedback-arxiv">​</a></h3><blockquote><p>Starting with a set of labeler-written prompts and prompts submitted through
the OpenAI API, we collect a dataset of labeler demonstrations of the desired
model behavior, which we use to fine-tune GPT-3 using supervised learning. We
then collect a dataset of rankings of model outputs, which we use to further
fine-tune this supervised model using reinforcement learning from human
feedback.</p></blockquote><h3 class="anchor anchorWithStickyNavbar_LWe7" id="training-a-helpful-and-harmless-assistant-with-reinforcement-learning-from-human-feedback-arxiv">Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback [<a href="https://arxiv.org/abs/2204.05862" target="_blank" rel="noopener noreferrer">ArXiv</a>]<a href="#training-a-helpful-and-harmless-assistant-with-reinforcement-learning-from-human-feedback-arxiv" class="hash-link" aria-label="Direct link to training-a-helpful-and-harmless-assistant-with-reinforcement-learning-from-human-feedback-arxiv" title="Direct link to training-a-helpful-and-harmless-assistant-with-reinforcement-learning-from-human-feedback-arxiv">​</a></h3><blockquote><p>We apply preference modeling and reinforcement learning from human feedback
(RLHF) to finetune language models to act as helpful and harmless assistants.
We find this alignment training improves performance on almost all NLP
evaluations, and is fully compatible with training for specialized skills such
as python coding and summarization.</p></blockquote><h3 class="anchor anchorWithStickyNavbar_LWe7" id="self-critiquing-models-for-assisting-human-evaluators-arxiv">Self-critiquing models for assisting human evaluators [<a href="https://arxiv.org/abs/2206.05802" target="_blank" rel="noopener noreferrer">ArXiv</a>]<a href="#self-critiquing-models-for-assisting-human-evaluators-arxiv" class="hash-link" aria-label="Direct link to self-critiquing-models-for-assisting-human-evaluators-arxiv" title="Direct link to self-critiquing-models-for-assisting-human-evaluators-arxiv">​</a></h3><blockquote><p>We fine-tune large language models to write natural language critiques
(natural language critical comments) using behavioral cloning. On a
topic-based summarization task, critiques written by our models help humans
find flaws in summaries that they would have otherwise missed. We study
scaling properties of critiquing with both topic-based summarization and
synthetic tasks. Finally, we motivate and introduce a framework for comparing
critiquing ability to generation and discrimination ability. These results are
a proof of concept for using AI-assisted human feedback to scale the
supervision of machine learning systems to tasks that are difficult for humans
to evaluate directly. We release our training datasets.</p></blockquote><h3 class="anchor anchorWithStickyNavbar_LWe7" id="is-reinforcement-learning-not-for-natural-language-processing-benchmarks-baselines-and-building-blocks-for-natural-language-policy-optimization-arxiv">Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization [<a href="https://arxiv.org/abs/2210.01241" target="_blank" rel="noopener noreferrer">ArXiv</a>]<a href="#is-reinforcement-learning-not-for-natural-language-processing-benchmarks-baselines-and-building-blocks-for-natural-language-policy-optimization-arxiv" class="hash-link" aria-label="Direct link to is-reinforcement-learning-not-for-natural-language-processing-benchmarks-baselines-and-building-blocks-for-natural-language-policy-optimization-arxiv" title="Direct link to is-reinforcement-learning-not-for-natural-language-processing-benchmarks-baselines-and-building-blocks-for-natural-language-policy-optimization-arxiv">​</a></h3><blockquote><p>We tackle the problem of aligning pre-trained large language models (LMs) with
human preferences. We present the GRUE (General Reinforced-language
Understanding Evaluation) benchmark, a set of 6 language generation tasks
which are supervised by reward functions which capture automated measures of
human preference. Finally, we introduce an easy-to-use, performant RL
algorithm, NLPO (Natural Language Policy Optimization) that learns to
effectively reduce the combinatorial action space in language generation. We
show that RL techniques are generally better than supervised methods at
aligning LMs to human preferences.</p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="generating-text-from-language-models">Generating Text From Language Models<a href="#generating-text-from-language-models" class="hash-link" aria-label="Direct link to Generating Text From Language Models" title="Direct link to Generating Text From Language Models">​</a></h2><p>A language model generates output text token by token, autoregressively. The
large search space of this task requires some method of narrowing down the set
of tokens to be considered in each step. This method, in turn, has a big impact
on the quality of the resulting text.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="rankgen-improving-text-generation-with-large-ranking-models-arxiv-github">RANKGEN: Improving Text Generation with Large Ranking Models [<a href="https://arxiv.org/abs/2205.09726" target="_blank" rel="noopener noreferrer">ArXiv</a>], [<a href="https://github.com/martiansideofthemoon/rankgen" target="_blank" rel="noopener noreferrer">GitHub</a>]<a href="#rankgen-improving-text-generation-with-large-ranking-models-arxiv-github" class="hash-link" aria-label="Direct link to rankgen-improving-text-generation-with-large-ranking-models-arxiv-github" title="Direct link to rankgen-improving-text-generation-with-large-ranking-models-arxiv-github">​</a></h3><blockquote><p>Given an input sequence (or prefix), modern language models often assign high
probabilities to output sequences that are repetitive, incoherent, or
irrelevant to the prefix; as such, model-generated text also contains such
artifacts. To address these issues we present RankGen, a 1.2B parameter
encoder model for English that scores model generations given a prefix.
RankGen can be flexibly incorporated as a scoring function in beam search and
used to decode from any pretrained language model.</p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="automatically-generating-instruction-data-for-training">Automatically Generating Instruction Data for Training<a href="#automatically-generating-instruction-data-for-training" class="hash-link" aria-label="Direct link to Automatically Generating Instruction Data for Training" title="Direct link to Automatically Generating Instruction Data for Training">​</a></h2><p>This line of work is about significantly reducing the need for manually
annotated data for the purpose of training
<a href="https://openai.com/blog/instruction-following/" target="_blank" rel="noopener noreferrer">instruction-aligned</a> language
models.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="self-instruct-aligning-language-model-with-self-generated-instructions-arxiv-github">SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions [<a href="https://arxiv.org/abs/2212.10560" target="_blank" rel="noopener noreferrer">ArXiv</a>], [<a href="https://github.com/yizhongw/self-instruct" target="_blank" rel="noopener noreferrer">GitHub</a>].<a href="#self-instruct-aligning-language-model-with-self-generated-instructions-arxiv-github" class="hash-link" aria-label="Direct link to self-instruct-aligning-language-model-with-self-generated-instructions-arxiv-github" title="Direct link to self-instruct-aligning-language-model-with-self-generated-instructions-arxiv-github">​</a></h3><blockquote><p>We introduce SELF-INSTRUCT, a framework for improving the
instruction-following capabilities of pretrained language models by
bootstrapping off its own generations. Our pipeline generates instruction,
input, and output samples from a language model, then prunes them before using
them to finetune the original model. Applying our method to vanilla GPT3, we
demonstrate a 33% absolute improvement over the original model on
SuperNaturalInstructions, on par with the performance of InstructGPT-0011,
which is trained with private user data and human annotations.</p></blockquote><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tuning-language-models-with-almost-no-human-labor-arxiv-github">Tuning Language Models with (Almost) No Human Labor. [<a href="https://arxiv.org/abs/2212.09689" target="_blank" rel="noopener noreferrer">ArXiv</a>], [<a href="https://github.com/orhonovich/unnatural-instructions" target="_blank" rel="noopener noreferrer">GitHub</a>].<a href="#tuning-language-models-with-almost-no-human-labor-arxiv-github" class="hash-link" aria-label="Direct link to tuning-language-models-with-almost-no-human-labor-arxiv-github" title="Direct link to tuning-language-models-with-almost-no-human-labor-arxiv-github">​</a></h3><blockquote><p>In this work, we introduce Unnatural Instructions: a large dataset of creative
and diverse instructions, collected with virtually no human labor. We collect
64,000 examples by prompting a language model with three seed examples of
instructions and eliciting a fourth. This set is then expanded by prompting
the model to rephrase each instruction, creating a total of approximately
240,000 examples of instructions, inputs, and outputs. Experiments show that
despite containing a fair amount of noise, training on Unnatural Instructions
rivals the effectiveness of training on open-source manually-curated datasets,
surpassing the performance of models such as T0++ and Tk-Instruct across
various benchmarks.</p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="uncertainty-estimation-of-language-model-outputs">Uncertainty Estimation of Language Model Outputs<a href="#uncertainty-estimation-of-language-model-outputs" class="hash-link" aria-label="Direct link to Uncertainty Estimation of Language Model Outputs" title="Direct link to Uncertainty Estimation of Language Model Outputs">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="teaching-models-to-express-their-uncertainty-in-words-arxiv">Teaching models to express their uncertainty in words [<a href="https://arxiv.org/abs/2205.14334" target="_blank" rel="noopener noreferrer">ArXiv</a>]<a href="#teaching-models-to-express-their-uncertainty-in-words-arxiv" class="hash-link" aria-label="Direct link to teaching-models-to-express-their-uncertainty-in-words-arxiv" title="Direct link to teaching-models-to-express-their-uncertainty-in-words-arxiv">​</a></h3><blockquote><p>We show that a GPT-3 model can learn to express uncertainty about its own
answers in natural language -- without use of model logits. When given a
question, the model generates both an answer and a level of confidence (e.g.
&quot;90% confidence&quot; or &quot;high confidence&quot;). These levels map to probabilities that
are well calibrated. The model also remains moderately calibrated under
distribution shift, and is sensitive to uncertainty in its own answers, rather
than imitating human examples.</p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="evidence-guided-text-generation">Evidence-Guided Text Generation<a href="#evidence-guided-text-generation" class="hash-link" aria-label="Direct link to Evidence-Guided Text Generation" title="Direct link to Evidence-Guided Text Generation">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="webgpt-browser-assisted-question-answering-with-human-feedback-arxiv">WebGPT: Browser-assisted question-answering with human feedback [<a href="https://arxiv.org/abs/2112.09332" target="_blank" rel="noopener noreferrer">ArXiv</a>]<a href="#webgpt-browser-assisted-question-answering-with-human-feedback-arxiv" class="hash-link" aria-label="Direct link to webgpt-browser-assisted-question-answering-with-human-feedback-arxiv" title="Direct link to webgpt-browser-assisted-question-answering-with-human-feedback-arxiv">​</a></h3><blockquote><p>We fine-tune GPT-3 to answer long-form questions using a text-based
web-browsing environment, which allows the model to search and navigate the
web. We are able to train models on the task using imitation learning, and
then optimize answer quality with human feedback. Models must collect
references while browsing in support of their answers. Our best model is
obtained by fine-tuning GPT-3 using behavior cloning, and then performing
rejection sampling against a reward model.</p></blockquote><h3 class="anchor anchorWithStickyNavbar_LWe7" id="teaching-language-models-to-support-answers-with-verified-quotes-arxiv">Teaching language models to support answers with verified quotes [<a href="https://arxiv.org/abs/2203.11147" target="_blank" rel="noopener noreferrer">ArXiv</a>]<a href="#teaching-language-models-to-support-answers-with-verified-quotes-arxiv" class="hash-link" aria-label="Direct link to teaching-language-models-to-support-answers-with-verified-quotes-arxiv" title="Direct link to teaching-language-models-to-support-answers-with-verified-quotes-arxiv">​</a></h3><blockquote><p>In this work we use RLHF to train &quot;open-book&quot; QA models that generate answers
whilst also citing specific evidence for their claims, which aids in the
appraisal of correctness. Supporting evidence is drawn from multiple documents
found via a search engine, or from a single user-provided document. However,
analysis on the adversarial TruthfulQA dataset shows why citation is only one
part of an overall strategy for safety and trustworthiness: not all claims
supported by evidence are true.</p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="reward-model-optimization">Reward Model Optimization<a href="#reward-model-optimization" class="hash-link" aria-label="Direct link to Reward Model Optimization" title="Direct link to Reward Model Optimization">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="scaling-laws-for-reward-model-overoptimization-arxiv-preceding-blogpost">Scaling Laws for Reward Model Overoptimization [<a href="https://arxiv.org/abs/2210.10760" target="_blank" rel="noopener noreferrer">ArXiv</a>], [<a href="https://openai.com/blog/measuring-goodharts-law/" target="_blank" rel="noopener noreferrer">Preceding Blogpost</a>]<a href="#scaling-laws-for-reward-model-overoptimization-arxiv-preceding-blogpost" class="hash-link" aria-label="Direct link to scaling-laws-for-reward-model-overoptimization-arxiv-preceding-blogpost" title="Direct link to scaling-laws-for-reward-model-overoptimization-arxiv-preceding-blogpost">​</a></h3><blockquote><p>In this work, we use a synthetic setup in which a fixed &quot;gold-standard&quot; reward
model plays the role of humans, providing labels used to train a proxy reward
model. We study how the gold reward model score changes as we optimize against
the proxy reward model using either reinforcement learning or best-of-n
sampling. We study the effect on this relationship of the size of the reward
model dataset. We explore the implications of these empirical results for
theoretical considerations in AI alignment.</p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="dialogue-oriented-rlhf">Dialogue-Oriented RLHF<a href="#dialogue-oriented-rlhf" class="hash-link" aria-label="Direct link to Dialogue-Oriented RLHF" title="Direct link to Dialogue-Oriented RLHF">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dynamic-planning-in-open-ended-dialogue-using-reinforcement-learning-arxiv">Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning [<a href="https://arxiv.org/abs/2208.02294" target="_blank" rel="noopener noreferrer">ArXiv</a>]<a href="#dynamic-planning-in-open-ended-dialogue-using-reinforcement-learning-arxiv" class="hash-link" aria-label="Direct link to dynamic-planning-in-open-ended-dialogue-using-reinforcement-learning-arxiv" title="Direct link to dynamic-planning-in-open-ended-dialogue-using-reinforcement-learning-arxiv">​</a></h3><blockquote><p>Building automated agents that can carry on rich open-ended conversations with
humans &quot;in the wild&quot; remains a formidable challenge. In this work we develop a
real-time, open-ended dialogue system that uses reinforcement learning (RL) to
power a bot&#x27;s conversational skill at scale. Trained using crowd-sourced data,
our novel system is able to substantially exceeds several metrics of interest
in a live experiment with real users of the Google Assistant.</p></blockquote><h3 class="anchor anchorWithStickyNavbar_LWe7" id="improving-alignment-of-dialogue-agents-via-targeted-human-judgements-arxiv">Improving alignment of dialogue agents via targeted human judgements [<a href="https://arxiv.org/abs/2209.14375" target="_blank" rel="noopener noreferrer">ArXiv</a>]<a href="#improving-alignment-of-dialogue-agents-via-targeted-human-judgements-arxiv" class="hash-link" aria-label="Direct link to improving-alignment-of-dialogue-agents-via-targeted-human-judgements-arxiv" title="Direct link to improving-alignment-of-dialogue-agents-via-targeted-human-judgements-arxiv">​</a></h3><blockquote><p>We present Sparrow, an information-seeking dialogue agent trained to be more
helpful, correct, and harmless compared to prompted language model baselines
First, to make our agent more helpful and harmless, we break down the
requirements for good dialogue into natural language rules the agent should
followy. Second, our agent provides evidence from sources supporting factual
claims when collecting preference judgements over model statements.Finally, we
conduct extensive analyses showing that though our model learns to follow our
rules it can exhibit distributional biases.</p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="reduce-harms-in-language-models">Reduce Harms in Language Models<a href="#reduce-harms-in-language-models" class="hash-link" aria-label="Direct link to Reduce Harms in Language Models" title="Direct link to Reduce Harms in Language Models">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="red-teaming-language-models-to-reduce-harms-methods-scaling-behaviors-and-lessons-learned-arxiv">Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned [<a href="https://arxiv.org/abs/2209.07858" target="_blank" rel="noopener noreferrer">ArXiv</a>]<a href="#red-teaming-language-models-to-reduce-harms-methods-scaling-behaviors-and-lessons-learned-arxiv" class="hash-link" aria-label="Direct link to red-teaming-language-models-to-reduce-harms-methods-scaling-behaviors-and-lessons-learned-arxiv" title="Direct link to red-teaming-language-models-to-reduce-harms-methods-scaling-behaviors-and-lessons-learned-arxiv">​</a></h3><blockquote><p>We investigate scaling behaviors for red teaming. We find that the RLHF models
are increasingly difficult to red team as they scale, and we find a flat trend
with scale for the other model types. We exhaustively describe our
instructions, processes, statistical methodologies, and uncertainty about red
teaming.</p></blockquote></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/Open-Assistant/docs/research"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Research</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/Open-Assistant/docs/research/search-based-qa"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Cohere Grounded QA</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#table-of-contents" class="table-of-contents__link toc-highlight">Table of Contents</a></li><li><a href="#reinforcement-learning-from-human-feedback" class="table-of-contents__link toc-highlight">Reinforcement Learning from Human Feedback</a><ul><li><a href="#fine-tuning-language-models-from-human-preferences-arxiv-github" class="table-of-contents__link toc-highlight">Fine-Tuning Language Models from Human Preferences [ArXiv], [GitHub]</a></li><li><a href="#learning-to-summarize-from-human-feedback-arxiv-github" class="table-of-contents__link toc-highlight">Learning to summarize from human feedback [ArXiv], [GitHub]</a></li><li><a href="#recursively-summarizing-books-with-human-feedback-arxiv" class="table-of-contents__link toc-highlight">Recursively Summarizing Books with Human Feedback [ArXiv]</a></li><li><a href="#training-language-models-to-follow-instructions-with-human-feedback-arxiv" class="table-of-contents__link toc-highlight">Training language models to follow instructions with human feedback [ArXiv]</a></li><li><a href="#training-a-helpful-and-harmless-assistant-with-reinforcement-learning-from-human-feedback-arxiv" class="table-of-contents__link toc-highlight">Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback [ArXiv]</a></li><li><a href="#self-critiquing-models-for-assisting-human-evaluators-arxiv" class="table-of-contents__link toc-highlight">Self-critiquing models for assisting human evaluators [ArXiv]</a></li><li><a href="#is-reinforcement-learning-not-for-natural-language-processing-benchmarks-baselines-and-building-blocks-for-natural-language-policy-optimization-arxiv" class="table-of-contents__link toc-highlight">Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization [ArXiv]</a></li></ul></li><li><a href="#generating-text-from-language-models" class="table-of-contents__link toc-highlight">Generating Text From Language Models</a><ul><li><a href="#rankgen-improving-text-generation-with-large-ranking-models-arxiv-github" class="table-of-contents__link toc-highlight">RANKGEN: Improving Text Generation with Large Ranking Models [ArXiv], [GitHub]</a></li></ul></li><li><a href="#automatically-generating-instruction-data-for-training" class="table-of-contents__link toc-highlight">Automatically Generating Instruction Data for Training</a><ul><li><a href="#self-instruct-aligning-language-model-with-self-generated-instructions-arxiv-github" class="table-of-contents__link toc-highlight">SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions [ArXiv], [GitHub].</a></li><li><a href="#tuning-language-models-with-almost-no-human-labor-arxiv-github" class="table-of-contents__link toc-highlight">Tuning Language Models with (Almost) No Human Labor. [ArXiv], [GitHub].</a></li></ul></li><li><a href="#uncertainty-estimation-of-language-model-outputs" class="table-of-contents__link toc-highlight">Uncertainty Estimation of Language Model Outputs</a><ul><li><a href="#teaching-models-to-express-their-uncertainty-in-words-arxiv" class="table-of-contents__link toc-highlight">Teaching models to express their uncertainty in words [ArXiv]</a></li></ul></li><li><a href="#evidence-guided-text-generation" class="table-of-contents__link toc-highlight">Evidence-Guided Text Generation</a><ul><li><a href="#webgpt-browser-assisted-question-answering-with-human-feedback-arxiv" class="table-of-contents__link toc-highlight">WebGPT: Browser-assisted question-answering with human feedback [ArXiv]</a></li><li><a href="#teaching-language-models-to-support-answers-with-verified-quotes-arxiv" class="table-of-contents__link toc-highlight">Teaching language models to support answers with verified quotes [ArXiv]</a></li></ul></li><li><a href="#reward-model-optimization" class="table-of-contents__link toc-highlight">Reward Model Optimization</a><ul><li><a href="#scaling-laws-for-reward-model-overoptimization-arxiv-preceding-blogpost" class="table-of-contents__link toc-highlight">Scaling Laws for Reward Model Overoptimization [ArXiv], [Preceding Blogpost]</a></li></ul></li><li><a href="#dialogue-oriented-rlhf" class="table-of-contents__link toc-highlight">Dialogue-Oriented RLHF</a><ul><li><a href="#dynamic-planning-in-open-ended-dialogue-using-reinforcement-learning-arxiv" class="table-of-contents__link toc-highlight">Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning [ArXiv]</a></li><li><a href="#improving-alignment-of-dialogue-agents-via-targeted-human-judgements-arxiv" class="table-of-contents__link toc-highlight">Improving alignment of dialogue agents via targeted human judgements [ArXiv]</a></li></ul></li><li><a href="#reduce-harms-in-language-models" class="table-of-contents__link toc-highlight">Reduce Harms in Language Models</a><ul><li><a href="#red-teaming-language-models-to-reduce-harms-methods-scaling-behaviors-and-lessons-learned-arxiv" class="table-of-contents__link toc-highlight">Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned [ArXiv]</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://ykilcher.com/open-assistant-discord" target="_blank" rel="noopener noreferrer" class="footer__link-item">OpenAssistant Contributors Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discord.com/invite/mVcgxMPD7e" target="_blank" rel="noopener noreferrer" class="footer__link-item">LAION Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://ykilcher.com/discord" target="_blank" rel="noopener noreferrer" class="footer__link-item">YK Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/LAION-AI/Open-Assistant" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://projects.laion.ai/Open-Assistant/docs/faq" target="_blank" rel="noopener noreferrer" class="footer__link-item">FAQ<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 laion.ai. Built with Docusaurus.</div></div></div></footer></div>
<script src="/Open-Assistant/assets/js/runtime~main.43d83866.js"></script>
<script src="/Open-Assistant/assets/js/main.2f8790aa.js"></script>
</body>
</html>